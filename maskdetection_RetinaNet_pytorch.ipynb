{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maskdetection_RetinaNet_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d73e38760d6b44fa85dcf89f9a293f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47a179da514f4dd58e2e4699b002e646",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0beecc2663f342a1a17fcb7947918389",
              "IPY_MODEL_4b2458665e614a4fbb562258b06a0a32",
              "IPY_MODEL_e9a98f1d524b41468e9ebd960fa846dd"
            ]
          }
        },
        "47a179da514f4dd58e2e4699b002e646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0beecc2663f342a1a17fcb7947918389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24b30a3e8c194d1d8f7888f88c2ec400",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c7f01a248f44d57868d5abcac7dcf7e"
          }
        },
        "4b2458665e614a4fbb562258b06a0a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3f67137345047e0a56964cd6874f87e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1c76bb8efb7421f9a7662601b7de677"
          }
        },
        "e9a98f1d524b41468e9ebd960fa846dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d107dd4eeb14405a930d3df754fceed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 107MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_888b8983d14a4637a32831a046f9d29c"
          }
        },
        "24b30a3e8c194d1d8f7888f88c2ec400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c7f01a248f44d57868d5abcac7dcf7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3f67137345047e0a56964cd6874f87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1c76bb8efb7421f9a7662601b7de677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d107dd4eeb14405a930d3df754fceed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "888b8983d14a4637a32831a046f9d29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Download"
      ],
      "metadata": {
        "id": "gFbAeezvYaYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksjVSZP9YLq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fbd776-0ec6-400f-cf49-26d0f6167536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Tutorial-Book-Utils'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 9), reused 18 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n",
            "Face Mask Detection.zip is done!\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
        "!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection\n",
        "!unzip -q Face\\ Mask\\ Detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data split"
      ],
      "metadata": {
        "id": "PCp-k-NEYd3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "print(len(os.listdir('annotations')))\n",
        "print(len(os.listdir('images')))\n",
        "\n",
        "!mkdir test_images\n",
        "!mkdir test_annotations\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "idx = random.sample(range(853), 170)\n",
        "\n",
        "for img in np.array(sorted(os.listdir('images')))[idx]:\n",
        "    shutil.move('images/'+img, 'test_images/'+img)\n",
        "\n",
        "for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n",
        "    shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n",
        "\n",
        "print(len(os.listdir('annotations')))\n",
        "print(len(os.listdir('images')))\n",
        "print(len(os.listdir('test_annotations')))\n",
        "print(len(os.listdir('test_images')))"
      ],
      "metadata": {
        "id": "GLJLvM_vYZ1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a93a82-9886-4237-dcd2-0bfffd17377d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "853\n",
            "853\n",
            "683\n",
            "683\n",
            "170\n",
            "170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset 정의"
      ],
      "metadata": {
        "id": "MuhXa2EcYozf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "def generate_box(obj):\n",
        "    \n",
        "    xmin = float(obj.find('xmin').text)\n",
        "    ymin = float(obj.find('ymin').text)\n",
        "    xmax = float(obj.find('xmax').text)\n",
        "    ymax = float(obj.find('ymax').text)\n",
        "    \n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "def generate_label(obj):\n",
        "\n",
        "    if obj.find('name').text == \"with_mask\":\n",
        "\n",
        "        return 1\n",
        "\n",
        "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
        "\n",
        "        return 2\n",
        "\n",
        "    return 0\n",
        "\n",
        "def generate_target(file): \n",
        "    with open(file) as f:\n",
        "        data = f.read()\n",
        "        soup = BeautifulSoup(data, \"html.parser\")\n",
        "        objects = soup.find_all(\"object\")\n",
        "\n",
        "        num_objs = len(objects)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for i in objects:\n",
        "            boxes.append(generate_box(i))\n",
        "            labels.append(generate_label(i))\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
        "        \n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        \n",
        "        return target\n",
        "\n",
        "def plot_image_from_output(img, annotation):\n",
        "    \n",
        "    img = img.cpu().permute(1,2,0)\n",
        "    \n",
        "    rects = []\n",
        "\n",
        "    for idx in range(len(annotation[\"boxes\"])):\n",
        "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
        "\n",
        "        if annotation['labels'][idx] == 0 :\n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
        "        \n",
        "        elif annotation['labels'][idx] == 1 :\n",
        "            \n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
        "            \n",
        "        else :\n",
        "        \n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
        "\n",
        "        rects.append(rect)\n",
        "\n",
        "    return img, rects\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.path = path\n",
        "        self.imgs = list(sorted(os.listdir(self.path)))\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_image = self.imgs[idx]\n",
        "        file_label = self.imgs[idx][:-3] + 'xml'\n",
        "        img_path = os.path.join(self.path, file_image)\n",
        "        \n",
        "        if 'test' in self.path:\n",
        "            label_path = os.path.join(\"test_annotations/\", file_label)\n",
        "        else:\n",
        "            label_path = os.path.join(\"annotations/\", file_label)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        target = generate_target(label_path)\n",
        "        \n",
        "        to_tensor = torchvision.transforms.ToTensor()\n",
        "\n",
        "        if self.transform:\n",
        "            # transform 에 boxes 를 넣어주고 결과물을 다시 target에 저장\n",
        "            img, transform_target = self.transform(np.array(img), np.array(target['boxes']))\n",
        "            target['boxes'] = torch.as_tensor(transform_target)\n",
        "\n",
        "        img = to_tensor(img)\n",
        "\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "dataset = MaskDataset('images/')\n",
        "test_dataset = MaskDataset('test_images/')\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "oeNOSc68YrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "X4N1dfwUaE0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "oVQp9ToTaGBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "e0633bfc-6cfc-4f84-f722-9a65e3737e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.3 MB 26 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 725 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.0\n",
            "  Downloading torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (3.10.0.2)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1+cu101) (7.1.2)\n",
            "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0+cu101 torchaudio-0.7.0 torchvision-0.8.1+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses",
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "id": "s5sYbryvbG3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95a14dad-7310-4a68-d6e6-b883c0fd3627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.11.1+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 3, pretrained=False, pretrained_backbone = True)"
      ],
      "metadata": {
        "id": "Kn6qOHnEbLmy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d73e38760d6b44fa85dcf89f9a293f76",
            "47a179da514f4dd58e2e4699b002e646",
            "0beecc2663f342a1a17fcb7947918389",
            "4b2458665e614a4fbb562258b06a0a32",
            "e9a98f1d524b41468e9ebd960fa846dd",
            "24b30a3e8c194d1d8f7888f88c2ec400",
            "2c7f01a248f44d57868d5abcac7dcf7e",
            "e3f67137345047e0a56964cd6874f87e",
            "e1c76bb8efb7421f9a7662601b7de677",
            "4d107dd4eeb14405a930d3df754fceed",
            "888b8983d14a4637a32831a046f9d29c"
          ]
        },
        "outputId": "8c44b72e-de7b-4500-a5e3-8d590ea2861f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d73e38760d6b44fa85dcf89f9a293f76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "num_epochs = 30\n",
        "retina.to(device)\n",
        "\n",
        "params = [p for p in retina.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "len_dataloader = len(data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start = time.time()\n",
        "  retina.train()\n",
        "\n",
        "  i = 0    \n",
        "  epoch_loss = 0\n",
        "\n",
        "  for images, targets in data_loader:\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    loss_dict = retina(images, targets) \n",
        "\n",
        "    losses = sum(loss for loss in loss_dict.values()) \n",
        "\n",
        "    i += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += losses \n",
        "  print(epoch_loss, f'time: {time.time() - start}')"
      ],
      "metadata": {
        "id": "LfAHI-X_bXag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c73ffdf-9b7d-4fc4-ed84-1cd092b242a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(272.5645, device='cuda:0', grad_fn=<AddBackward0>) time: 746.9106512069702\n",
            "tensor(272.9682, device='cuda:0', grad_fn=<AddBackward0>) time: 738.5532429218292\n",
            "tensor(257.6504, device='cuda:0', grad_fn=<AddBackward0>) time: 738.0636625289917\n",
            "tensor(230.0723, device='cuda:0', grad_fn=<AddBackward0>) time: 744.6297628879547\n",
            "tensor(176.7085, device='cuda:0', grad_fn=<AddBackward0>) time: 748.5412681102753\n",
            "tensor(120.0592, device='cuda:0', grad_fn=<AddBackward0>) time: 749.6666271686554\n",
            "tensor(95.4801, device='cuda:0', grad_fn=<AddBackward0>) time: 750.3524057865143\n",
            "tensor(84.9150, device='cuda:0', grad_fn=<AddBackward0>) time: 750.6308369636536\n",
            "tensor(75.8966, device='cuda:0', grad_fn=<AddBackward0>) time: 747.2963533401489\n",
            "tensor(73.8647, device='cuda:0', grad_fn=<AddBackward0>) time: 743.8908584117889\n",
            "tensor(66.7899, device='cuda:0', grad_fn=<AddBackward0>) time: 743.7662451267242\n",
            "tensor(60.9344, device='cuda:0', grad_fn=<AddBackward0>) time: 743.8375313282013\n",
            "tensor(56.0084, device='cuda:0', grad_fn=<AddBackward0>) time: 744.0978119373322\n",
            "tensor(51.8193, device='cuda:0', grad_fn=<AddBackward0>) time: 743.7344949245453\n",
            "tensor(46.5741, device='cuda:0', grad_fn=<AddBackward0>) time: 744.1085894107819\n",
            "tensor(46.3505, device='cuda:0', grad_fn=<AddBackward0>) time: 743.7109098434448\n",
            "tensor(41.2390, device='cuda:0', grad_fn=<AddBackward0>) time: 744.0707013607025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model, img, threshold):\n",
        "  model.eval()\n",
        "  preds = model(img)\n",
        "  for id in range(len(preds)):\n",
        "    idx_list = []\n",
        "\n",
        "    for idx, score in enumerate(preds[id][\"scores\"]):\n",
        "      if score > threshold:\n",
        "        idx_list.append(idx)\n",
        "    \n",
        "    preds[id][\"boxes\"] = preds[id][\"boxes\"][idx_list]\n",
        "    preds[id][\"labels\"] = preds[id][\"labels\"][idx_list]\n",
        "    preds[id][\"scores\"] = preds[id][\"scores\"][idx_list]\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "_oJsrmEF5sXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "labels = []\n",
        "preds_adj_all = []\n",
        "annot_all = []\n",
        "\n",
        "for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
        "    im = list(img.to(device) for img in im)\n",
        "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
        "\n",
        "    for t in annot:\n",
        "        labels += t['labels']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds_adj = make_prediction(retina, im, 0.5)\n",
        "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
        "        preds_adj_all.append(preds_adj)\n",
        "        annot_all.append(annot)"
      ],
      "metadata": {
        "id": "OqMJNNXpDJYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 8\n",
        "ncols = 2\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*4))\n",
        "\n",
        "batch_i = 0\n",
        "for im, annot in test_data_loader:\n",
        "    pos = batch_i * 4 + 1\n",
        "    for sample_i in range(len(im)) :\n",
        "        \n",
        "        img, rects = plot_image_from_output(im[sample_i], annot[sample_i])\n",
        "        axes[(pos)//2, 1-((pos)%2)].imshow(img)\n",
        "        for rect in rects:\n",
        "            axes[(pos)//2, 1-((pos)%2)].add_patch(rect)\n",
        "        \n",
        "        img, rects = plot_image_from_output(im[sample_i], preds_adj_all[batch_i][sample_i])\n",
        "        axes[(pos)//2, 1-((pos+1)%2)].imshow(img)\n",
        "        for rect in rects:\n",
        "            axes[(pos)//2, 1-((pos+1)%2)].add_patch(rect)\n",
        "\n",
        "        pos += 2\n",
        "\n",
        "    batch_i += 1\n",
        "    if batch_i == 4:\n",
        "        break\n",
        "\n",
        "# xtick, ytick 제거\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "colnames = ['True', 'Pred']\n",
        "\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.set_title(colnames[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKmAAr8pD-ij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}